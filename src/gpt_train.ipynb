{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AKSHAT SHAW\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cr</th>\n",
       "      <th>Ni</th>\n",
       "      <th>Mo</th>\n",
       "      <th>W</th>\n",
       "      <th>N</th>\n",
       "      <th>Nb</th>\n",
       "      <th>C</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mn</th>\n",
       "      <th>...</th>\n",
       "      <th>Test Solution</th>\n",
       "      <th>[Cl-] M</th>\n",
       "      <th>pH</th>\n",
       "      <th>Test Method</th>\n",
       "      <th>Scan Rate mV/s</th>\n",
       "      <th>Heat treatment</th>\n",
       "      <th>Microstructures</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Material class</th>\n",
       "      <th>combine_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.71</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>300 ppm NaCl, pH 7.8</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Potentiodynamic Polarization with exposed area...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>S30403 (304L)</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;S&gt; Test Solution: 0       300 ppm NaCl, pH 7....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.71</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>500 ppm NaCl, pH 7.8</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Potentiodynamic Polarization with exposed area...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>S30403 (304L)</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;S&gt; Test Solution: 0       300 ppm NaCl, pH 7....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.71</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000 ppm NaCl, pH 7.8</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Potentiodynamic Polarization with exposed area...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>S30403 (304L)</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;S&gt; Test Solution: 0       300 ppm NaCl, pH 7....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.71</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5000 ppm NaCl, pH 7.8</td>\n",
       "      <td>0.0856</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Potentiodynamic Polarization with exposed area...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>S30403 (304L)</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;S&gt; Test Solution: 0       300 ppm NaCl, pH 7....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.71</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Seawater Natural</td>\n",
       "      <td>0.5460</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Potentiodynamic Polarization with exposed area...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>S30403 (304L)</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;S&gt; Test Solution: 0       300 ppm NaCl, pH 7....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fe    Cr    Ni   Mo    W    N   Nb     C   Si   Mn  ...  \\\n",
       "0  69.71  18.0  10.0  0.0  0.0  0.0  0.0  0.03  1.0  1.0  ...   \n",
       "1  69.71  18.0  10.0  0.0  0.0  0.0  0.0  0.03  1.0  1.0  ...   \n",
       "2  69.71  18.0  10.0  0.0  0.0  0.0  0.0  0.03  1.0  1.0  ...   \n",
       "3  69.71  18.0  10.0  0.0  0.0  0.0  0.0  0.03  1.0  1.0  ...   \n",
       "4  69.71  18.0  10.0  0.0  0.0  0.0  0.0  0.03  1.0  1.0  ...   \n",
       "\n",
       "           Test Solution  [Cl-] M   pH  \\\n",
       "0   300 ppm NaCl, pH 7.8   0.0051  7.8   \n",
       "1   500 ppm NaCl, pH 7.8   0.0086  7.8   \n",
       "2  1000 ppm NaCl, pH 7.8   0.0171  7.8   \n",
       "3  5000 ppm NaCl, pH 7.8   0.0856  7.8   \n",
       "4       Seawater Natural   0.5460  8.2   \n",
       "\n",
       "                                         Test Method  Scan Rate mV/s  \\\n",
       "0  Potentiodynamic Polarization with exposed area...             0.1   \n",
       "1  Potentiodynamic Polarization with exposed area...             0.1   \n",
       "2  Potentiodynamic Polarization with exposed area...             0.1   \n",
       "3  Potentiodynamic Polarization with exposed area...             0.1   \n",
       "4  Potentiodynamic Polarization with exposed area...             0.1   \n",
       "\n",
       "   Heat treatment  Microstructures       Comments  Material class  \\\n",
       "0   Not Available    Not Available  S30403 (304L)               1   \n",
       "1   Not Available    Not Available  S30403 (304L)               1   \n",
       "2   Not Available    Not Available  S30403 (304L)               1   \n",
       "3   Not Available    Not Available  S30403 (304L)               1   \n",
       "4   Not Available    Not Available  S30403 (304L)               1   \n",
       "\n",
       "                                        combine_text  \n",
       "0  <S> Test Solution: 0       300 ppm NaCl, pH 7....  \n",
       "1  <S> Test Solution: 0       300 ppm NaCl, pH 7....  \n",
       "2  <S> Test Solution: 0       300 ppm NaCl, pH 7....  \n",
       "3  <S> Test Solution: 0       300 ppm NaCl, pH 7....  \n",
       "4  <S> Test Solution: 0       300 ppm NaCl, pH 7....  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"..\\Code\\pit_cleaned_final.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Fe', 'Cr', 'Ni', 'Mo', 'W', 'N', 'Nb', 'C', 'Si', 'Mn', 'Cu', 'P', 'S',\n",
       "        'Al', 'V', 'Ta', 'Re', 'Ce', 'Ti', 'Co', 'B', 'Mg', 'Y', 'Gd',\n",
       "        'Epit, mV (SCE)', 'Test Temp (C)', '[Cl-] M', 'pH', 'Scan Rate mV/s',\n",
       "        'Material class'],\n",
       "       dtype='object'),\n",
       " Index(['Test Solution', 'Test Method', 'Heat treatment', 'Microstructures',\n",
       "        'Comments', 'combine_text'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = data.select_dtypes(include=\"number\").columns\n",
    "cat = data.select_dtypes(exclude=\"number\").columns\n",
    "num, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[num] = scaler.fit_transform(data[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 569 rows (70%)\n",
      "Validation set: 71 rows (15%)\n",
      "Test set: 72 rows (15%)\n"
     ]
    }
   ],
   "source": [
    "# Creating the data loaders\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, temp = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Second split: separate val and test from temp\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the sizes\n",
    "print(f\"Train set: {len(train)} rows (70%)\")\n",
    "print(f\"Validation set: {len(val)} rows (15%)\")\n",
    "print(f\"Test set: {len(test)} rows (15%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the dataloader for the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Dataset_txt(Dataset):\n",
    "    def __init__(self, data, tokenizer,txt_col, target = None, max_length=None, pad_token_id=50256):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.txt_col = txt_col\n",
    "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[f\"{txt_col}\"]]\n",
    "        \n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "        \n",
    "        self.encoded_texts = [\n",
    "            encoded_text[:self.max_length] for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "        \n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        # label = self.data.iloc[index][f\"{self.target}\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long)\n",
    "            # torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = Dataset_txt(\n",
    "    data = train,\n",
    "    tokenizer=tokenizer,\n",
    "    txt_col=\"combine_text\",\n",
    "    max_length=512\n",
    ")\n",
    "val_data = Dataset_txt(\n",
    "    data = val,\n",
    "    tokenizer=tokenizer,\n",
    "    txt_col=\"combine_text\",\n",
    "    max_length=512\n",
    ")\n",
    "test_data = Dataset_txt(\n",
    "    data = test,\n",
    "    tokenizer=tokenizer,\n",
    "    txt_col=\"combine_text\",\n",
    "    max_length=512\n",
    ")\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0 \n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Num(Dataset):\n",
    "    def __init__(self, data, numerical_features):\n",
    "        self.data = data\n",
    "        self.numerical_features = numerical_features\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "           torch.tensor(self.data.iloc[index].loc[self.numerical_features], dtype=torch.float),\n",
    "            torch.tensor(self.data.iloc[index].loc[\"Epit, mV (SCE)\"], dtype=torch.float)\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = Dataset_Num(\n",
    "    data = train,\n",
    "    numerical_features = num\n",
    ")\n",
    "val_num = Dataset_Num(\n",
    "    data = val,\n",
    "    numerical_features = num\n",
    ")\n",
    "test_num = Dataset_Num(\n",
    "    data = test,\n",
    "    numerical_features = num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_loader = DataLoader(\n",
    "    dataset=train_num,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_num_loader = DataLoader(\n",
    "    dataset=val_num,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")    \n",
    "test_num_loader = DataLoader(\n",
    "    dataset=test_num,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 30]) torch.Size([8])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([8, 30]) torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKSHAT SHAW\\AppData\\Local\\Temp\\ipykernel_19588\\4259121505.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  torch.tensor(self.data.iloc[index].loc[self.numerical_features], dtype=torch.float),\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_num_loader:\n",
    " print(x.shape, y.shape)\n",
    " break\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in test_num_loader:\n",
    " print(x.shape, y.shape)\n",
    " break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the orignal model\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "BASE_CONFIG = {\n",
    " \"vocab_size\": 50257, \n",
    " \"context_length\": 1024, \n",
    " \"drop_rate\": 0.0, \n",
    " \"qkv_bias\": True \n",
    "}\n",
    "model_configs = {\n",
    " \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    " \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    " \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    " \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    " \"vocab_size\": 50257,\n",
    " \"context_length\": 256, # We shorten the context length from 1,024 to 256 tokens. Original GPT-2 has a context length of 1,024 tokens.\n",
    " \"emb_dim\": 768,\n",
    " \"n_heads\": 12,\n",
    " \"n_layers\": 12, \n",
    " \"drop_rate\": 0.1, \n",
    " \"qkv_bias\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2, load_weights_into_gpt\n",
    "from gpt import GPTModel\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings, params = download_and_load_gpt2(                \n",
    "        model_size=model_size, models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "gpt_model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(gpt_model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.eval()\n",
    "for param in gpt_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 32 # we take a 32dim output form the gpt model for the representation of text.\n",
    "\n",
    "gpt_model.out_head = torch.nn.Linear(\n",
    " in_features=BASE_CONFIG[\"emb_dim\"], \n",
    " out_features=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training the final layer norm and last transformer block trainable\n",
    "for param in gpt_model.trf_blocks[-1].parameters():\n",
    " param.requires_grad = True\n",
    "for param in gpt_model.final_norm.parameters():\n",
    " param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitModel(torch.nn.Module):\n",
    "    def __init__(self, gpt_model, embedding_dim = 32, numerical_features= len(num)):\n",
    "        super().__init__()\n",
    "        self.num_classes = embedding_dim + numerical_features\n",
    "        self.gpt_model = gpt_model\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.num_classes, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_num, x_text):\n",
    "        x = self.gpt_model(x_text)\n",
    "        x = x[:, -1, :]\n",
    "        x = torch.cat((x, x_num), dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PitModel(gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### testing random words\n",
    "# inputs = tokenizer.encode(\"Do you have time\")\n",
    "# inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "# print(\"Inputs:\", inputs)\n",
    "# print(\"Inputs dimensions:\", inputs.shape)\n",
    "# with torch.no_grad():\n",
    "#  outputs = model(inputs)\n",
    "# print(\"Outputs:\\n\", outputs)\n",
    "# print(\"Outputs dimensions:\", outputs.shape)\n",
    "# probas = outputs[:, -1, :]\n",
    "# print(\"E(pit) mV:\", probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_regression_metrics(data_num_loader, data_txt_loader, model, device, num_batches=None):\n",
    "#     model.eval()\n",
    "#     total_mae, total_mse, num_examples = 0, 0, 0\n",
    "    \n",
    "#     if num_batches is None:\n",
    "#         num_batches = min(len(data_num_loader), len(data_txt_loader))\n",
    "#     else:\n",
    "#         num_batches = min(num_batches, min(len(data_num_loader), len(data_txt_loader)))\n",
    "    \n",
    "#     # Create iterator for both loaders\n",
    "#     num_iterator = iter(data_num_loader)\n",
    "#     txt_iterator = iter(data_txt_loader)\n",
    "    \n",
    "#     for i in range(num_batches):\n",
    "#         try:\n",
    "#             # Get batches from both loaders\n",
    "#             num_batch, target_batch = next(num_iterator)\n",
    "#             txt_batch, _ = next(txt_iterator)  # Ignore the targets from text loader\n",
    "            \n",
    "#             # Move to device\n",
    "#             num_batch = num_batch.to(device)\n",
    "#             txt_batch = txt_batch.to(device)\n",
    "#             target_batch = target_batch.to(device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 # Forward pass with both inputs\n",
    "#                 predictions = model(num_batch, txt_batch)\n",
    "                \n",
    "#                 # Ensure predictions and targets have same shape\n",
    "#                 if predictions.shape != target_batch.shape:\n",
    "#                     predictions = predictions.squeeze()\n",
    "                \n",
    "#                 # Calculate absolute errors and squared errors\n",
    "#                 absolute_errors = torch.abs(predictions - target_batch)\n",
    "#                 squared_errors = (predictions - target_batch) ** 2\n",
    "                \n",
    "#                 # Sum up errors\n",
    "#                 total_mae += absolute_errors.sum().item()\n",
    "#                 total_mse += squared_errors.sum().item()\n",
    "#                 num_examples += target_batch.shape[0]\n",
    "        \n",
    "#         except StopIteration:\n",
    "#             # Handle case where one loader is exhausted before the other\n",
    "#             break\n",
    "    \n",
    "#     # Calculate final metrics\n",
    "#     mae = total_mae / num_examples\n",
    "#     rmse = (total_mse / num_examples) ** 0.5\n",
    "    \n",
    "#     return {'mae': mae, 'rmse': rmse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PitModel(\n",
       "  (gpt_model): GPTModel(\n",
       "    (tok_emb): Embedding(50257, 768)\n",
       "    (pos_emb): Embedding(1024, 768)\n",
       "    (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "    (trf_blocks): Sequential(\n",
       "      (0): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (6): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (7): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (8): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (9): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (10): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (11): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm()\n",
       "    (out_head): Linear(in_features=768, out_features=32, bias=True)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=62, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "# train_accuracy = calc_regression_metrics(\n",
    "#  train_loader, model, device, num_batches=10\n",
    "# )\n",
    "\n",
    "# test_accuracy = calc_regression_metrics(\n",
    "#  test_loader, model, device, num_batches=10\n",
    "# )\n",
    "# train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def calc_loss_batch(num_batch, txt_batch, target_batch, model, device):\n",
    "    num_batch = num_batch.to(device)\n",
    "    txt_batch = txt_batch.to(device)\n",
    "    target_batch = target_batch.float().to(device)\n",
    "    \n",
    "    # Forward pass through the model with both numerical and text inputs\n",
    "    predictions = model(num_batch, txt_batch)\n",
    "    \n",
    "    # Ensure predictions and targets have the same shape\n",
    "    if predictions.shape != target_batch.shape:\n",
    "        predictions = predictions.squeeze()\n",
    "    \n",
    "    # Calculate both MAE and MSE losses\n",
    "    mae_loss = F.l1_loss(predictions, target_batch)\n",
    "    mse_loss = F.mse_loss(predictions, target_batch)\n",
    "    \n",
    "    return {'mae': mae_loss, 'mse': mse_loss, 'predictions': predictions, 'targets': target_batch}\n",
    "\n",
    "def calc_loss_loader(data_num_loader, data_txt_loader, model, device, num_batches=None):\n",
    "    total_mae = 0.\n",
    "    total_mse = 0.\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # Check if both loaders have data\n",
    "    if len(data_num_loader) == 0 or len(data_txt_loader) == 0:\n",
    "        return {'mae': float(\"nan\"), 'rmse': float(\"nan\"), 'r2': float(\"nan\")}\n",
    "    \n",
    "    # Determine number of batches to process\n",
    "    if num_batches is None:\n",
    "        num_batches = min(len(data_num_loader), len(data_txt_loader))\n",
    "    else: \n",
    "        num_batches = min(num_batches, min(len(data_num_loader), len(data_txt_loader)))\n",
    "    \n",
    "    # Create iterators for both loaders\n",
    "    num_iterator = iter(data_num_loader)\n",
    "    txt_iterator = iter(data_txt_loader)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        try:\n",
    "            # Get batches from both loaders\n",
    "            num_batch, target_batch = next(num_iterator)\n",
    "            txt_batch = next(txt_iterator)  # Ignore targets from text loader\n",
    "            \n",
    "            batch_results = calc_loss_batch(num_batch, txt_batch, target_batch, model, device)\n",
    "            \n",
    "            total_mae += batch_results['mae'].item()\n",
    "            total_mse += batch_results['mse'].item()\n",
    "            \n",
    "            # Collect predictions and targets for R² calculation\n",
    "            all_predictions.append(batch_results['predictions'].cpu().detach())\n",
    "            all_targets.append(batch_results['targets'].cpu().detach())\n",
    "        \n",
    "        except StopIteration:\n",
    "            # Handle case where one loader is exhausted before the other\n",
    "            break\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    num_processed_batches = len(all_predictions)\n",
    "    if num_processed_batches == 0:\n",
    "        return {'mae': float(\"nan\"), 'rmse': float(\"nan\"), 'r2': float(\"nan\")}\n",
    "        \n",
    "    mae = total_mae / num_processed_batches\n",
    "    rmse = (total_mse / num_processed_batches) ** 0.5\n",
    "    \n",
    "    # Calculate R² score\n",
    "    all_predictions = torch.cat(all_predictions).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    \n",
    "    # R² = 1 - (sum of squared residuals / total sum of squares)\n",
    "    ss_res = ((all_targets - all_predictions) ** 2).sum()\n",
    "    ss_tot = ((all_targets - all_targets.mean()) ** 2).sum()\n",
    "    \n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKSHAT SHAW\\AppData\\Local\\Temp\\ipykernel_19588\\4259121505.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  torch.tensor(self.data.iloc[index].loc[self.numerical_features], dtype=torch.float),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'mae': 0.7246540188789368,\n",
       "  'rmse': 1.0222703725160143,\n",
       "  'r2': -0.18423569202423096},\n",
       " {'mae': 0.9952387027442455,\n",
       "  'rmse': 1.2478205530860675,\n",
       "  'r2': -0.22752594947814941})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    train_loss = calc_loss_loader(train_num_loader, train_loader, model, device, num_batches=8)\n",
    "    test_loss = calc_loss_loader(train_num_loader, test_loader, model, device, num_batches=8)\n",
    "train_loss, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makshatshaw47\u001b[0m (\u001b[33makshatshaw47-iit-roorkee\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(\n",
    "    model, train_num_loader, train_txt_loader, val_num_loader, val_txt_loader, optimizer, device,\n",
    "    num_epochs, eval_freq, eval_iter, loss_function=\"mse\",\n",
    "    project_name=None, run_name=None):\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(project=project_name, name=run_name)\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    wandb.config.update({\n",
    "        \"epochs\": num_epochs,\n",
    "        \"eval_frequency\": eval_freq,\n",
    "        \"eval_iterations\": eval_iter,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"device\": device,\n",
    "        \"model_name\": model.__class__.__name__\n",
    "    })\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_maes, val_maes = [], []\n",
    "    train_rmses, val_rmses = [], []\n",
    "    train_r2s, val_r2s = [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    \n",
    "    # Create iterators for the training loaders\n",
    "    num_train_iter = iter(train_num_loader)\n",
    "    txt_train_iter = iter(train_txt_loader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_mae_loss = 0\n",
    "        epoch_mse_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Reset iterators at the beginning of each epoch\n",
    "        num_train_iter = iter(train_num_loader)\n",
    "        txt_train_iter = iter(train_txt_loader)\n",
    "        \n",
    "        # Determine number of batches for this epoch\n",
    "        num_batches = min(len(train_num_loader), len(train_txt_loader))\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            try:\n",
    "                # Get batches from both loaders\n",
    "                num_batch, target_batch = next(num_train_iter)\n",
    "                txt_batch = next(txt_train_iter)  # Ignore targets from text loader\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_dict = calc_loss_batch(\n",
    "                    num_batch, txt_batch, target_batch, model, device\n",
    "                )\n",
    "                # For backward pass, you can choose either MAE or MSE or a combination\n",
    "                loss = loss_dict[f'{loss_function}']\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track batch-level metrics\n",
    "                epoch_mae_loss += loss_dict['mae'].item()\n",
    "                epoch_mse_loss += loss_dict['mse'].item()\n",
    "                batch_count += 1\n",
    "                examples_seen += num_batch.shape[0]\n",
    "                global_step += 1\n",
    "                \n",
    "                # Log batch metrics\n",
    "                wandb.log({\n",
    "                    \"batch_mae\": loss_dict['mae'].item(),\n",
    "                    \"batch_mse\": loss_dict['mse'].item(),\n",
    "                    \"batch_rmse\": loss_dict['mse'].item() ** 0.5,\n",
    "                    \"examples_seen\": examples_seen,\n",
    "                    \"global_step\": global_step\n",
    "                }, step=global_step)\n",
    "                \n",
    "                if global_step % eval_freq == 0:\n",
    "                    # Evaluate model\n",
    "                    train_metrics = calc_loss_loader(train_num_loader, train_txt_loader, model, device, eval_iter)\n",
    "                    val_metrics = calc_loss_loader(val_num_loader, val_txt_loader, model, device, eval_iter)\n",
    "                    \n",
    "                    # Store metrics\n",
    "                    train_losses.append(train_metrics['mae'])\n",
    "                    val_losses.append(val_metrics['mae'])\n",
    "                    train_maes.append(train_metrics['mae'])\n",
    "                    val_maes.append(val_metrics['mae'])\n",
    "                    train_rmses.append(train_metrics['rmse'])\n",
    "                    val_rmses.append(val_metrics['rmse'])\n",
    "                    train_r2s.append(train_metrics['r2'])\n",
    "                    val_r2s.append(val_metrics['r2'])\n",
    "                    \n",
    "                    # Log evaluation metrics\n",
    "                    wandb.log({\n",
    "                        \"train_mae\": train_metrics['mae'],\n",
    "                        \"val_mae\": val_metrics['mae'],\n",
    "                        \"train_rmse\": train_metrics['rmse'],\n",
    "                        \"val_rmse\": val_metrics['rmse'],\n",
    "                        \"train_r2\": train_metrics['r2'],\n",
    "                        \"val_r2\": val_metrics['r2'],\n",
    "                        \"epoch\": epoch + 1\n",
    "                    }, step=global_step)\n",
    "                    \n",
    "                    print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                          f\"Train MAE {train_metrics['mae']:.3f}, \"\n",
    "                          f\"Val MAE {val_metrics['mae']:.3f}, \"\n",
    "                          f\"Train R² {train_metrics['r2']:.3f}, \"\n",
    "                          f\"Val R² {val_metrics['r2']:.3f}\")\n",
    "            \n",
    "            except StopIteration:\n",
    "                # Handle case where one loader is exhausted before the other\n",
    "                break\n",
    "        \n",
    "        # Calculate and log metrics at epoch end\n",
    "        train_metrics = calc_loss_loader(train_num_loader, train_txt_loader, model, device, eval_iter)\n",
    "        val_metrics = calc_loss_loader(val_num_loader, val_txt_loader, model, device, eval_iter)\n",
    "        \n",
    "        # Log epoch-level metrics\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"epoch_avg_mae\": epoch_mae_loss / batch_count if batch_count > 0 else float('nan'),\n",
    "            \"epoch_avg_mse\": epoch_mse_loss / batch_count if batch_count > 0 else float('nan'),\n",
    "            \"epoch_avg_rmse\": (epoch_mse_loss / batch_count) ** 0.5 if batch_count > 0 else float('nan'),\n",
    "            \"train_mae\": train_metrics['mae'],\n",
    "            \"val_mae\": val_metrics['mae'],\n",
    "            \"train_rmse\": train_metrics['rmse'],\n",
    "            \"val_rmse\": val_metrics['rmse'],\n",
    "            \"train_r2\": train_metrics['r2'],\n",
    "            \"val_r2\": val_metrics['r2'],\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        }, step=global_step)\n",
    "        \n",
    "        print(f\"Training MAE: {train_metrics['mae']:.4f} | \", end=\"\")\n",
    "        print(f\"Validation MAE: {val_metrics['mae']:.4f} | \", end=\"\")\n",
    "        print(f\"Training RMSE: {train_metrics['rmse']:.4f} | \", end=\"\")\n",
    "        print(f\"Validation RMSE: {val_metrics['rmse']:.4f} | \", end=\"\")\n",
    "        print(f\"Training R²: {train_metrics['r2']:.4f} | \", end=\"\")\n",
    "        print(f\"Validation R²: {val_metrics['r2']:.4f}\")\n",
    "    \n",
    "    # Close wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    results = {\n",
    "        'train_losses': train_losses, \n",
    "        'val_losses': val_losses,\n",
    "        'train_maes': train_maes,\n",
    "        'val_maes': val_maes,\n",
    "        'train_rmses': train_rmses,\n",
    "        'val_rmses': val_rmses,\n",
    "        'train_r2s': train_r2s,\n",
    "        'val_r2s': val_r2s,\n",
    "        'examples_seen': examples_seen\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKSHAT SHAW\\AppData\\Local\\Temp\\ipykernel_19588\\4259121505.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  torch.tensor(self.data.iloc[index].loc[self.numerical_features], dtype=torch.float),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train MAE 0.733, Val MAE 0.726, Train R² -0.095, Val R² -0.084\n",
      "Training MAE: 0.7288 | Validation MAE: 0.6996 | Training RMSE: 0.9710 | Validation RMSE: 0.9616 | Training R²: -0.0852 | Validation R²: -0.0897\n",
      "Ep 2 (Step 000100): Train MAE 0.693, Val MAE 0.677, Train R² -0.066, Val R² 0.020\n",
      "Training MAE: 0.7126 | Validation MAE: 0.6788 | Training RMSE: 0.9156 | Validation RMSE: 0.9026 | Training R²: 0.0027 | Validation R²: 0.0385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer(\n\u001b[0;32m      8\u001b[0m     model,train_num_loader, train_loader, val_num_loader, val_loader, optimizer, device,\n\u001b[0;32m      9\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, eval_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     10\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-corr-txt-only\u001b[39m\u001b[38;5;124m\"\u001b[39m, run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-run-3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     13\u001b[0m execution_time_minutes \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "Cell \u001b[1;32mIn[33], line 59\u001b[0m, in \u001b[0;36mtrainer\u001b[1;34m(model, train_num_loader, train_txt_loader, val_num_loader, val_txt_loader, optimizer, device, num_epochs, eval_freq, eval_iter, loss_function, project_name, run_name)\u001b[0m\n\u001b[0;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Track batch-level metrics\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m epoch_mae_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     60\u001b[0m epoch_mse_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     61\u001b[0m batch_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "results = trainer(\n",
    "    model,train_num_loader, train_loader, val_num_loader, val_loader, optimizer, device,\n",
    "    num_epochs=10, eval_freq=100, eval_iter=10,\n",
    "    project_name=\"gpt2-corr-txt-only\", run_name=\"test-run-3\"\n",
    ")\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
